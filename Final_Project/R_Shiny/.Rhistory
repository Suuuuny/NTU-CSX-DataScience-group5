mutant[2]
Phenotype[4,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant
Phenotype[4,2]
url=read_html(Phenotype[4,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant[1]
FindMutant <- function(x){
url=read_html(Phenotype[x,2])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(2)
write.csv(FindMutant(2),file="test.txt")
FindMutant(2)[1]
write.csv(FindMutant(2),file=FindMutant(2)[1]+".txt")
FindMutant(2)[1]
paste0(FindMutant(2)[1],".txt")
write.csv(FindMutant(2),file=paste0(FindMutant(2)[1],".txt"))
write.csv(FindMutant(2)[2],file=paste0(FindMutant(2)[1],".txt"))
Phenotype[2]
sapply(Phenotype[2], write)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(FindMutant(x)[1],".txt"))
}
sapply(Phenotype[2], write)
Phenotype[2]
Phenotype[,2]
sapply(Phenotype[,2], write)
write(x)
write(2)
sapply(c(1:68), write)
c(1:68)
Phenotype[,2]
sapply(c(1:68), write)
sapply(c(1:68), FindMutant)
sapply(1, FindMutant)
sapply(2, FindMutant)
sapply(50, FindMutant)
sapply(40, FindMutant)
sapply(45, FindMutant)
paste0(FindMutant(2)[1],".txt")
sapply(46, FindMutant)
sapply(48, FindMutant)
sapply(49, FindMutant)
sapply(50, FindMutant)
sapply(c(1:10), write)
sapply(c(11:20), write)
sapply(c(21:30), write)
sapply(c(21:25), write)
sapply(c(21:22), write)
sapply(c(21), write)
sapply(c(22), write)
sapply(c(23), write)
sapply(c(24:30), write)
Phenotype[,2]
findMutant(Phenotype[1,2])
FindMutant(Phenotype[1,2])
Phenotype[1,2]
Phenotype[4,2]
View(Phenotype)
Phenotype[,1]
ct = 1
sapply(Phenotype[,1],function(x){
paste(ct,"_",x)
ct+=1
})
sapply(Phenotype[,1],function(x){
return((ct,"_",x))
ct = ct +1
})
Phenotype <- cbind(c(1:68),Phenotype)
View(Phenotype)
Phenotype[1,]
Phenotype[,1]
Phenotype[1,1]
FindMutant <- function(x){
url=read_html(Phenotype[x,3])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(1)
FindMutant(2)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],FindMutant(x)[1],".txt"))
}
write(1)
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
}
write(1)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],"_",FindMutant(x)[1],".txt"))
}
write(1)
sapply(c(1:68), write)
write(4)
write(5)
write(6)
sapply(c(7:15), write)
sapply(c(16:20), write)
sapply(c(21:25), write)
sapply(c(20:25), write)
sapply(c(22:25), write)
sapply(c(23:25), write)
sapply(c(26:35), write)
sapply(c(36:45), write)
sapply(c(42:45), write)
sapply(c(43:45), write)
sapply(c(46:55), write)
sapply(c(56:65), write)
sapply(c(66:68), write)
data <- as.factor(iris)
iris
data <- as.factor(iris3)
iris3
library(apriori)
install.packages(apriori)
install.packages("apriori")
#載入套件
library(neuralnet)
install.packages("neuralnet")
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
View(data)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
x
y <- logistic(x) + rnorm(100, sd = 0.2)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#圖解BP
plot(bpn)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Loading the required packages
require(monmlp)
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
install.packages("kerasR")
iris
data <- iris
View(data)
dataset
datasets::airquality
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
print(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
install.packages("nnet")
library("nnet")
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#(4)預測
#test組執行預測
prediction <- predict(nnetM, testdata, type = 'class')
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
data <- iris
View(data)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
cm
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
View(testdata)
View(traindata)
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
gset <- getGEO("GSE19983", GSEMatrix =TRUE, AnnotGPL=FALSE)
install.packages("Biobase")
library(rvest)
library(magrittr)
library(httr)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822")
View(test)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text %>% html_nodes(.,"h1")
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".date") %>% html_text()
print("hello")
source("https://bioconductor.org/biocLite.R")
biocLite("Rbowtie")
ibrary(swirl)
library(swirl)
source("https://wush978.github.io/R/init-swirl.R")
library(swirl)
library(swirl)
swirl()
x <- c(10.)
x <- c(10.4,5.6,3.1,6.4)
x
x
c(x,x)
c(1, 2, 3) - c(2, 4, 6)
print("XD")
source('~/.active-rstudio-document', echo=TRUE)
print("XD")
print("XD")
print("XD")
print("XD")
print("XD3")
print("XD3")
source('~/.active-rstudio-document', echo=TRUE)
print("XD2")
print("XD")
print("XD")
print("XD3")
print("XD1")
print("XD3")
source('~/.active-rstudio-document', echo=TRUE)
print("XD")
print("XD1")
print("XD2")
print("XD3")
print("XD2")
print("XD3")
setwd("~/GitHub/NTU-CSX-DataScience-group5/Final_Project/Sentiment")
library(jiebaRD)
library(jiebaR)
## 引用程式碼 : http://blog.sina.com.cn/s/blog_c368a6290102y4bg.html
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos、neg及weight
rm(pos)
rm(neg)
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
w1<-worker()
rm(weight)
new_user_word(w1,user)
# 要算情緒的檔案從這邊丟入
Data <- read.csv("FB_result/Di_report.csv")
# 要算情緒的檔案從這邊丟入
Data <- read.csv("FaceBook_report.csv")
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 文字清理
# 選擇post欄位(輸入要做情緒分析的資料) >> 更改 Data$ ???
docs <- Corpus(VectorSource(Data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
View(posneg)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
View(seg)
# 計算情感分數
sentiment_point <- sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
View(docs)
View(seg)
library(jiebaRD)
library(jiebaR)
## 引用程式碼 : http://blog.sina.com.cn/s/blog_c368a6290102y4bg.html
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos、neg及weight
rm(pos)
rm(neg)
w1<-worker()
rm(weight)
new_user_word(w1,user)
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
setwd("~/GitHub/NTU-CSX-DataScience-group5/Final_Project/Sentiment")
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
w1<-worker()
new_user_word(w1,user)
# 要算情緒的檔案從這邊丟入
Data <- read.csv("FaceBook_report.csv")
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 文字清理
# 選擇post欄位(輸入要做情緒分析的資料) >> 更改 Data$ ???
docs <- Corpus(VectorSource(Data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
View(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
View(seg)
seg[1]
length(seg[0])
length(seg[1])
length(seg[2])
seg[0]
seg[1]
seg[2]
View(seg)
View(seg)
# 計算情感分數
sentiment_point <- sapply(seg,function(d){
print(d)
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
print(length(d))
# 計算情感分數
sentiment_point <- sapply(seg,function(d){
print(length(d))
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
# 計算情感分數
sentiment_point <- sapply(seg,function(d){
if(length(d)==0){
return(NA)
}
print(length(d))
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
# 算分時候可能出現NA(此文章沒有正面也沒有負面)將NA改成0.5
sentiment_point[sentiment_point %>% is.na] = 0.5
# 算完分數的資料放回dataframe
Data <- cbind(Data,sentiment_point)
# 此時可以輸出Data這個結果
write.csv(Data,"FaceBookAPI-Taipei.csv")
setwd("~/GitHub/NTU-CSX-DataScience-group5/Final_Project/R_Shiny")
shiny::runApp()
runApp()
setwd("~/GitHub/NTU-CSX-DataScience-group5/Final_Project/R_Shiny")
FB_Taipei <- read.csv("FaceBookAPI-Taipei.csv",fileEncoding = "Big5")
View(FB_Taipei)
runApp()
runApp()
